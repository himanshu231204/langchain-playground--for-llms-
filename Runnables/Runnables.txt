These notes combine **official documentation ideas + practical usage + comparisons** and are **detailed and thorough** for deep learning or revision. ğŸ“˜

---

âœ… What Runnables are
âœ… Runnable core interface methods
âœ… Common Runnable types
âœ… Runnable composition (Sequential / Parallel / Branch)
âœ… How Agents and Chains relate
âœ… Runnable vs Chain vs LCEL
âœ… Streaming, Async, Batching
âœ… Internal mechanics & practical tips
---
# **Runnables in LangChain â€” Detailed Notes (.txt format)**

---

## **1) What Is a Runnable?**

**Definition:**
A **Runnable** is the **core standard primitive in LangChain** â€” a *generic executable unit* that takes an input, does something, and returns an output.
It defines a **uniform interface** that all core components (LLMs, prompts, output parsers, retrievers, tools, functions, chains) follow. ([LangChain Reference][1])

**Why LangChain uses Runnables:**
âœ” Provides a single consistent API for many components
âœ” Makes chaining or composing steps easy
âœ” Adds support for streaming, batching, async execution
âœ” Enables flexible workflows (parallel execution, branching, conditional logic)
âœ” Helps build complex AI flows modularly instead of hard-coding sequences. ([LangChain Reference][1])

---

## **2) Runnable Core Interface**

Every Runnable supports:

```
.invoke(input)     â†’ run once â†’ returns result
.ainvoke(input)    â†’ async invoke
.batch(inputs)    â†’ run multiple inputs
.abatch(inputs)   â†’ async batch
.stream(input)    â†’ streaming output
.astream(input)   â†’ async streaming
```

These methods provide a **uniform way to execute logic** regardless of what the runnable actually is. ([LangChain Reference][2])

**Key idea:**
Runnables donâ€™t just run. They support **batching, streaming, and async operations** by default.

---

## **3) Main Types of Runnables**

Hereâ€™s a rundown of the most common Runnable primitives:

---

### **3.1 RunnableLambda**

Converts a normal Python/JS function into a Runnable such that it can be part of a chain.

Example idea:

```python
from langchain_core.runnables import RunnableLambda
add_one = RunnableLambda(lambda x: x + 1)
result = add_one.invoke(10)  # â†’ 11
```

Wraps custom logic as a runnable unit. ([LangChain Reference][3])

---

### **3.2 RunnableSequence**

A **sequence/chain** of steps executed one after the other.

Instead of writing:

```
step1(data)
step2(step1_output)
step3(step2_output)
```

You write:

```
chain = prompt | llm | parser
```

This creates a **RunnableSequence**: input flows through steps. ([LangChain Reference][4])

---

### **3.3 RunnableParallel**

Runs multiple runnables **in parallel on the same input**, and returns a mapping of results.

Example concept:

```
{
  "summary": summary_chain,
  "sentiment": sentiment_chain
}
```

Input goes into both branches simultaneously. ([LangChain Reference][5])

---

### **3.4 RunnableBranch**

A **conditional runnable** that routes execution based on a test.

Example concept:

```
if length > 300:
    use long-summary
else:
    use short-summary
```

This is similar to `if-else` logic inside a pipeline. ([linkedin.com][6])

---

### **3.5 RunnablePassthrough**

Does nothing â€” simply passes the input forward unchanged.

Useful for:

* Debugging
* Serving as placeholder logic
* Combining with parallel execution for keys that donâ€™t transform input. ([Medium][7])

---

### **3.6 (Other Advanced Types)**

These are not always required but useful:

âœ” RunnableGenerator â€” for custom streaming logic
âœ” RunnableWithFallbacks â€” fallback behavior
âœ” RunnableWithMessageHistory â€” manage conversation history
âœ” RunnableWithRetry â€” add retry logic
âœ” RunnableWithTimeout â€” enforce execution limits ([Medium][8])

---

## **4) Composition â€” Building Workflows**

Runnables shine because you can **compose them easily**.

### **4.1 Sequential Composition**

Connect runnables end-to-end:

```
prompt | model | parser
```

Equivalent to:

```
parser(model(prompt(input)))
```

---

### **4.2 Parallel Composition**

```
{
  "summary": summary_chain,
  "quiz": quiz_chain
}
```

â†’ both chains run on input
â†’ combined results returned as dictionary. ([LangChain Reference][5])

---

### **4.3 Mixed Composition**

You can combine sequence + parallel:

```
prompt
  â†“
parallel (summary, sentiment)
  â†“
merge/parser
```

This yields more complex pipelines where results from parallel paths can be aggregated. ([linkedin.com][6])

---

## **5) LCEL â€” LangChain Expression Language**

LCEL is the **new declarative way** to build pipelines using Runnables.
It uses familiar syntax like `|` (pipe) and `dict` literals to compose runnables more naturally:

```
chain = prompt | llm | parser
```

LCEL is basically a **syntactic sugar** that compiles down to RunnableSequence / RunnableParallel internally. ([langchain-opentutorial.gitbook.io][9])

---

## **6) Runnables vs Old Chains vs Agents**

### **Old Chains**

Traditionally, LangChain had many different Chain classes (LLMChain, SQLDatabaseChain, etc.).
Drawbacks:

* Harder to unify
* Harder to compose
* Limited streaming support

---

### **Runnables**

Unified interface:

* Works with prompt templates
* Works with LLM models
* Works with output parsers
* Works with custom functions

This makes workflows more modular and composable. ([Medium][10])

---

### **Agents**

Agents (higher-level decision systems that call tools) **also use Runnables internally**.

An AgentExecutor itself is a Runnable that:

* receives input
* makes decisions
* runs tools
* returns outputs.
  Thus, the entire agent becomes an executable runnable pipeline.

---

## **7) Streaming, Async & Batching**

Modern AI applications often benefit from partial outputs and parallel processing.

Runnables provide built-in support for:
âœ” streaming responses (chunk by chunk)
âœ” asynchronous execution
âœ” batch execution across multiple inputs
âœ” nested streaming + parallelism ([LangChain Reference][2])

This is a big upgrade over older designs where streaming had to be implemented manually.

---

## **8) Practical Usage Patterns**

### **Custom Logic**

Use RunnableLambda to wrap any business logic and drop it into a chain.

---

### **Conditional Logic**

Use RunnableBranch when what you do next depends on the output:

```
if model_response contains long text: summarize
else: keep original
```

---

### **Parallel Tasks**

Use RunnableParallel for tasks like:

* summarization
* sentiment analysis
* keyword extraction
  in one pass.

---

## **9) Benefits of Runnables**

ğŸ“Œ **Modularity** â€” split work into smaller, reusable steps
ğŸ“Œ **Composability** â€” easy to connect runnables
ğŸ“Œ **Flexibility** â€” control logic paths
ğŸ“Œ **Performance benefits** â€” parallel execution
ğŸ“Œ **Streaming & Async** â€” built-in support
ğŸ“Œ **Unified interface** â€” makes everything consistent ([DEV Community][11])

---

## **10) Summary**

```
Runnable = input â†’ do something â†’ output
```

By standardizing the interface across different components, LangChain made building complex AI systems easier, more robust, and more maintainable. ([LangChain Reference][1])

---

## **Recommended Steps to Practice**

âœ” Build simple chains using `|` operator
âœ” Wrap custom functions with RunnableLambda
âœ” Experiment with RunnableParallel
âœ” Try conditional flows with RunnableBranch
âœ” Use streaming for real-time outputs
âœ” Explore async pipelines

---


## ğŸ”· Types of Runnables in LangChain

In LangChain, **Runnables** are categorized into two main types:

```
1ï¸âƒ£ Primitive Runnables
2ï¸âƒ£ Task-Specific Runnables
```

This classification helps you understand:

* Which ones are **low-level building blocks**
* Which ones are **high-level ready-made components**

---

# ğŸ§± 1ï¸âƒ£ Primitive Runnables

Primitive Runnables are the **core foundational building blocks**.

They donâ€™t perform a specific AI task like summarization or retrieval.
Instead, they provide **execution logic and composition capabilities**.

Think of them like:

> ğŸ—ï¸ LEGO blocks used to build workflows.

---

## ğŸ”¹ Common Primitive Runnables

---

### 1ï¸âƒ£ RunnableLambda

Wraps a normal Python function into a Runnable.

Purpose:

* Add custom logic
* Preprocessing
* Postprocessing
* Business logic

Example:

```
Input â†’ Python Function â†’ Output
```

---

### 2ï¸âƒ£ RunnableSequence

Runs multiple Runnables **in order (step-by-step)**.

Created automatically when using:

```
chain = A | B | C
```

Flow:

```
Input â†’ A â†’ B â†’ C â†’ Output
```

---

### 3ï¸âƒ£ RunnableParallel

Runs multiple Runnables **at the same time** on the same input.

Flow:

```
         â”Œâ”€ Chain A
Input â”€â”€â”€â”¤
         â””â”€ Chain B
```

Returns:

```
{
  "A": resultA,
  "B": resultB
}
```

---

### 4ï¸âƒ£ RunnableBranch

Implements conditional logic (if-else routing).

Flow:

```
Input
  â”‚
  â”œâ”€ If condition â†’ Chain A
  â””â”€ Else â†’ Chain B
```

Used for:

* Smart routing
* Tool selection
* Decision systems

---

### 5ï¸âƒ£ RunnablePassthrough

Does nothing. Just forwards input.

Useful when:

* You need original input preserved
* Combining parallel outputs

---

### 6ï¸âƒ£ RunnableGenerator

Used for custom streaming behavior.

---

### 7ï¸âƒ£ Wrapper Runnables

These add execution behavior:

* with_retry()
* with_fallbacks()
* with_timeout()
* with_config()

These modify how a runnable behaves.

---

## ğŸ”¹ What Primitive Runnables Do

They control:

âœ” Execution order
âœ” Parallelism
âœ” Branching
âœ” Error handling
âœ” Streaming
âœ” Retry
âœ” Async

They are **infrastructure-level components**.

---

# ğŸ¯ 2ï¸âƒ£ Task-Specific Runnables

These are **higher-level Runnables** built to perform specific AI tasks.

They usually wrap models, prompts, retrievers, tools, or chains.

Think of them as:

> ğŸ§  Functional AI components ready to do real tasks.

---

## ğŸ”¹ Examples of Task-Specific Runnables

---

### 1ï¸âƒ£ Chat Models / LLMs

Example:

* ChatOpenAI
* Other chat models

These are Runnables because:

```
Input (prompt/messages) â†’ Model â†’ Output
```

---

### 2ï¸âƒ£ Prompt Templates

PromptTemplate and ChatPromptTemplate are Runnables.

They transform structured input into formatted prompts.

```
Input variables â†’ formatted prompt
```

---

### 3ï¸âƒ£ Output Parsers

StrOutputParser, JSONOutputParser, etc.

They convert model output into:

* String
* JSON
* Structured data

---

### 4ï¸âƒ£ Retrievers

Vector store retrievers are Runnables.

```
Query â†’ Relevant Documents
```

Used in:

* RAG systems

---

### 5ï¸âƒ£ Tools

Tools (used in agents) are Runnables.

```
Input â†’ Tool execution â†’ Output
```

---

### 6ï¸âƒ£ Chains

Modern chains are built using Runnables.

Example:

```
Prompt | LLM | Parser
```

That whole chain is a Runnable.

---

### 7ï¸âƒ£ Agents

Agents internally use Runnables for:

* Decision making
* Tool calling
* Planning

AgentExecutor itself behaves like a Runnable.

---

# ğŸ†š Primitive vs Task-Specific Runnables

| Feature                    | Primitive       | Task-Specific |
| -------------------------- | --------------- | ------------- |
| Level                      | Low-level       | High-level    |
| Purpose                    | Execution logic | AI task logic |
| Example                    | RunnableLambda  | ChatOpenAI    |
| Controls flow?             | Yes             | No            |
| Performs AI task?          | No              | Yes           |
| Used to compose pipelines? | Yes             | Yes           |

---

## ğŸ§  Simple Mental Model

Primitive Runnables = HOW to run
Task-Specific Runnables = WHAT to run

Example:

```
Prompt (task-specific)
   â†“
LLM (task-specific)
   â†“
Parser (task-specific)

Sequence operator | (primitive)
```

---

# ğŸ”¥ Real Example

```
clean_text (Primitive)
   â†“
Prompt (Task-specific)
   â†“
LLM (Task-specific)
   â†“
Parser (Task-specific)
   â†“
Branch (Primitive)
```

Primitive Runnables control execution.
Task-Specific Runnables perform AI work.

---



**END of Notes**
